# ğŸš€ PRODUCTION READINESS CHECKLIST
## AnÃ¡lise de ProntidÃ£o para Deploy em ProduÃ§Ã£o

**Data:** 07 de Novembro de 2025
**Projeto:** theoldshinobi-site (revista-cms-api)
**VersÃ£o:** 1.0.0
**Status Atual:** ğŸŸ¡ **NÃƒO PRONTO PARA PRODUÃ‡ÃƒO**

---

## ğŸ“Š RESUMO EXECUTIVO

### Status Geral: 40% Pronto

| Categoria | Status | Progresso | Prioridade |
|-----------|--------|-----------|------------|
| **SeguranÃ§a** | ğŸŸ¢ Bom | 85% | âœ… Completo |
| **Arquitetura** | ğŸŸ¢ Bom | 90% | âœ… Completo |
| **Testes** | ğŸ”´ CrÃ­tico | 0% | âš ï¸ BLOQUEADOR |
| **CI/CD** | ğŸ”´ CrÃ­tico | 0% | âš ï¸ BLOQUEADOR |
| **ContainerizaÃ§Ã£o** | ğŸ”´ CrÃ­tico | 0% | âš ï¸ BLOQUEADOR |
| **Monitoramento** | ğŸ”´ CrÃ­tico | 10% | âš ï¸ BLOQUEADOR |
| **DocumentaÃ§Ã£o API** | ğŸŸ¡ MÃ©dio | 30% | ğŸ”¶ Importante |
| **Database Ops** | ğŸ”´ CrÃ­tico | 20% | âš ï¸ BLOQUEADOR |
| **Backup/DR** | ğŸ”´ CrÃ­tico | 0% | âš ï¸ BLOQUEADOR |

### âš ï¸ BLOQUEADORES CRÃTICOS (5)

Estes itens **DEVEM** ser resolvidos antes de qualquer deploy em produÃ§Ã£o:

1. **Testes Automatizados** - 0% de cobertura
2. **CI/CD Pipeline** - Inexistente
3. **Docker/ContainerizaÃ§Ã£o** - NÃ£o configurado
4. **Database Migrations** - Scripts manuais apenas
5. **Monitoramento e Alertas** - Inexistente

---

## ğŸ”´ CATEGORIA 1: BLOQUEADORES CRÃTICOS

### 1.1 Testes Automatizados (Severidade: CRÃTICA)

**Status Atual:** ğŸ”´ 0% de cobertura

**Problema:**
- Nenhum teste unitÃ¡rio implementado
- Nenhum teste de integraÃ§Ã£o
- Nenhum teste E2E
- Script de teste retorna erro: `"Error: no test specified" && exit 1`

**Impacto:**
- âŒ ImpossÃ­vel garantir qualidade do cÃ³digo
- âŒ RegressÃµes nÃ£o detectadas
- âŒ RefatoraÃ§Ãµes arriscadas
- âŒ Deploy sem confianÃ§a

**O que precisa ser feito:**

#### 1.1.1 Configurar Framework de Testes
```bash
npm install --save-dev jest supertest @types/jest
npm install --save-dev @faker-js/faker
```

#### 1.1.2 Estrutura de Testes NecessÃ¡ria
```
src/
â”œâ”€â”€ __tests__/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ AuthService.test.js
â”‚   â”‚   â”‚   â””â”€â”€ IssueService.test.js
â”‚   â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”‚   â”œâ”€â”€ UserRepository.test.js
â”‚   â”‚   â”‚   â””â”€â”€ IssueRepository.test.js
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ pagination.test.js
â”‚   â”‚       â””â”€â”€ logger.test.js
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ auth.integration.test.js
â”‚   â”‚   â”œâ”€â”€ issues.integration.test.js
â”‚   â”‚   â””â”€â”€ publishers.integration.test.js
â”‚   â””â”€â”€ e2e/
â”‚       â”œâ”€â”€ auth-flow.e2e.test.js
â”‚       â””â”€â”€ issue-crud.e2e.test.js
â”œâ”€â”€ __mocks__/
â”‚   â”œâ”€â”€ database.js
â”‚   â””â”€â”€ logger.js
â””â”€â”€ test-setup.js
```

#### 1.1.3 ConfiguraÃ§Ã£o Jest (jest.config.js)
```javascript
module.exports = {
  testEnvironment: 'node',
  coverageDirectory: 'coverage',
  collectCoverageFrom: [
    'src/**/*.js',
    '!src/index.js',
    '!src/**/__tests__/**'
  ],
  coverageThreshold: {
    global: {
      branches: 80,
      functions: 80,
      lines: 80,
      statements: 80
    }
  },
  testMatch: ['**/__tests__/**/*.test.js'],
  setupFilesAfterEnv: ['<rootDir>/src/test-setup.js']
};
```

#### 1.1.4 Atualizar package.json
```json
{
  "scripts": {
    "test": "jest --verbose",
    "test:watch": "jest --watch",
    "test:coverage": "jest --coverage",
    "test:unit": "jest --testPathPattern=unit",
    "test:integration": "jest --testPathPattern=integration --runInBand",
    "test:e2e": "jest --testPathPattern=e2e --runInBand"
  }
}
```

#### 1.1.5 Meta de Cobertura MÃ­nima
- **UnitÃ¡rios:** 90% dos Services e Repositories
- **IntegraÃ§Ã£o:** 80% dos endpoints da API
- **E2E:** Fluxos crÃ­ticos (auth, CRUD principal)

**EsforÃ§o Estimado:** 40-60 horas
**Prioridade:** ğŸ”´ CRÃTICA (Bloqueador #1)

---

### 1.2 CI/CD Pipeline (Severidade: CRÃTICA)

**Status Atual:** ğŸ”´ Inexistente

**Problema:**
- Nenhum arquivo de CI/CD configurado
- Deploy manual propenso a erros
- Sem testes automatizados no pipeline
- Sem validaÃ§Ã£o de qualidade antes do merge

**Impacto:**
- âŒ Deploy manual inconsistente
- âŒ Risco de deploy de cÃ³digo quebrado
- âŒ Sem rollback automÃ¡tico
- âŒ Tempo de deploy longo e propenso a erros

**O que precisa ser feito:**

#### 1.2.1 GitHub Actions Workflow

Criar `.github/workflows/ci.yml`:
```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

env:
  NODE_VERSION: '18.x'
  POSTGRES_VERSION: '15'

jobs:
  # ====================
  # JOB 1: Lint e Format
  # ====================
  lint:
    name: Lint & Format Check
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run ESLint
        run: npm run lint

      - name: Check Prettier formatting
        run: npm run format:check

  # ====================
  # JOB 2: Security Scan
  # ====================
  security:
    name: Security Audit
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Run npm audit
        run: npm audit --audit-level=moderate

      - name: Run Snyk security scan
        uses: snyk/actions/node@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --severity-threshold=high

  # ====================
  # JOB 3: Unit Tests
  # ====================
  test-unit:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: [lint]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        run: npm run test:unit

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./coverage/lcov.info
          flags: unittests

  # ====================
  # JOB 4: Integration Tests
  # ====================
  test-integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [lint]

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: revista_test
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Setup test database
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: revista_test
          DB_USER: test_user
          DB_PASSWORD: test_password
        run: npm run db:init

      - name: Run integration tests
        env:
          NODE_ENV: test
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: revista_test
          DB_USER: test_user
          DB_PASSWORD: test_password
          JWT_SECRET: test_secret_min_32_chars_required_here
        run: npm run test:integration

  # ====================
  # JOB 5: Build Docker Image
  # ====================
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [test-unit, test-integration, security]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/revista-cms-api:latest
            ${{ secrets.DOCKER_USERNAME }}/revista-cms-api:${{ github.sha }}
          cache-from: type=registry,ref=${{ secrets.DOCKER_USERNAME }}/revista-cms-api:buildcache
          cache-to: type=registry,ref=${{ secrets.DOCKER_USERNAME }}/revista-cms-api:buildcache,mode=max

  # ====================
  # JOB 6: Deploy to Production
  # ====================
  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [build]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://api.theoldshinobi.com

    steps:
      - name: Deploy to production server
        uses: appleboy/ssh-action@v1.0.0
        with:
          host: ${{ secrets.PROD_HOST }}
          username: ${{ secrets.PROD_USERNAME }}
          key: ${{ secrets.PROD_SSH_KEY }}
          script: |
            cd /opt/revista-cms-api
            docker-compose pull
            docker-compose up -d --no-deps --build api
            docker-compose exec -T api npm run db:migrate
```

#### 1.2.2 Secrets NecessÃ¡rios no GitHub

Configure em: **Settings â†’ Secrets and variables â†’ Actions**

```
DOCKER_USERNAME
DOCKER_PASSWORD
SNYK_TOKEN
CODECOV_TOKEN
PROD_HOST
PROD_USERNAME
PROD_SSH_KEY
```

#### 1.2.3 Branch Protection Rules

Configure em: **Settings â†’ Branches â†’ Branch protection rules**

Para branch `main`:
- âœ… Require a pull request before merging
- âœ… Require approvals (mÃ­nimo 1)
- âœ… Require status checks to pass before merging:
  - lint
  - security
  - test-unit
  - test-integration
- âœ… Require branches to be up to date before merging
- âœ… Do not allow bypassing the above settings

**EsforÃ§o Estimado:** 16-24 horas
**Prioridade:** ğŸ”´ CRÃTICA (Bloqueador #2)

---

### 1.3 Docker e ContainerizaÃ§Ã£o (Severidade: CRÃTICA)

**Status Atual:** ğŸ”´ NÃ£o configurado

**Problema:**
- Nenhum Dockerfile
- Nenhum docker-compose.yml
- Deploy inconsistente entre ambientes
- DifÃ­cil replicar ambiente de produÃ§Ã£o

**Impacto:**
- âŒ "Funciona na minha mÃ¡quina" syndrome
- âŒ Setup complexo para novos devs
- âŒ Ambientes inconsistentes
- âŒ Deploy lento e manual

**O que precisa ser feito:**

#### 1.3.1 Dockerfile Multi-Stage

Criar `Dockerfile`:
```dockerfile
# ====================
# Stage 1: Build
# ====================
FROM node:18-alpine AS builder

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies (including dev)
RUN npm ci

# Copy source code
COPY . .

# Run linting and validation
RUN npm run lint && npm run format:check

# ====================
# Stage 2: Production
# ====================
FROM node:18-alpine

# Create non-root user
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

WORKDIR /app

# Install production dependencies only
COPY package*.json ./
RUN npm ci --only=production && \
    npm cache clean --force

# Copy source from builder
COPY --from=builder --chown=nodejs:nodejs /app/src ./src
COPY --from=builder --chown=nodejs:nodejs /app/database ./database
COPY --from=builder --chown=nodejs:nodejs /app/scripts ./scripts

# Create uploads directory
RUN mkdir -p uploads/covers uploads/pdfs && \
    chown -R nodejs:nodejs uploads

# Switch to non-root user
USER nodejs

# Expose port
EXPOSE 3000

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
  CMD node -e "require('http').get('http://localhost:3000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"

# Start application
CMD ["node", "src/index.js"]
```

#### 1.3.2 .dockerignore

Criar `.dockerignore`:
```
node_modules
npm-debug.log
.env
.env.*
!.env.example
.git
.gitignore
README.md
*.md
.vscode
.idea
coverage
.nyc_output
.DS_Store
uploads/*
!uploads/.gitkeep
```

#### 1.3.3 docker-compose.yml para Desenvolvimento

Criar `docker-compose.yml`:
```yaml
version: '3.9'

services:
  # ====================
  # PostgreSQL Database
  # ====================
  db:
    image: postgres:15-alpine
    container_name: revista-db
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=C"
    ports:
      - "${DB_PORT}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - revista-network

  # ====================
  # API Application
  # ====================
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: revista-api
    restart: unless-stopped
    environment:
      NODE_ENV: ${NODE_ENV:-production}
      PORT: ${PORT:-3000}
      DB_HOST: db
      DB_PORT: 5432
      DB_NAME: ${DB_NAME}
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
      JWT_SECRET: ${JWT_SECRET}
      BCRYPT_SALT_ROUNDS: ${BCRYPT_SALT_ROUNDS:-12}
      ALLOWED_ORIGINS: ${ALLOWED_ORIGINS}
    ports:
      - "${PORT:-3000}:3000"
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    depends_on:
      db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - revista-network

  # ====================
  # Redis Cache (Opcional)
  # ====================
  redis:
    image: redis:7-alpine
    container_name: revista-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - revista-network

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local

networks:
  revista-network:
    driver: bridge
```

#### 1.3.4 docker-compose.prod.yml

Criar `docker-compose.prod.yml`:
```yaml
version: '3.9'

services:
  api:
    image: ${DOCKER_REGISTRY}/revista-cms-api:${VERSION:-latest}
    restart: always
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
      rollback_config:
        parallelism: 1
        delay: 5s
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  db:
    restart: always
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
```

#### 1.3.5 Scripts de Deploy

Criar `scripts/deploy.sh`:
```bash
#!/bin/bash
set -e

echo "ğŸš€ Starting deployment..."

# Pull latest images
docker-compose -f docker-compose.yml -f docker-compose.prod.yml pull

# Stop old containers
docker-compose -f docker-compose.yml -f docker-compose.prod.yml down

# Start new containers
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# Run database migrations
docker-compose exec -T api npm run db:migrate

# Health check
sleep 10
if curl -f http://localhost:3000/health; then
  echo "âœ… Deployment successful!"
else
  echo "âŒ Health check failed!"
  docker-compose -f docker-compose.yml -f docker-compose.prod.yml logs api
  exit 1
fi
```

**EsforÃ§o Estimado:** 12-16 horas
**Prioridade:** ğŸ”´ CRÃTICA (Bloqueador #3)

---

### 1.4 Database Migrations (Severidade: CRÃTICA)

**Status Atual:** ğŸ”´ Scripts SQL manuais apenas

**Problema:**
- Apenas `init.sql` para setup inicial
- Sem versionamento de schema
- Sem rollback de alteraÃ§Ãµes
- DifÃ­cil sincronizar entre ambientes

**Impacto:**
- âŒ AlteraÃ§Ãµes de schema arriscadas
- âŒ Sem histÃ³rico de mudanÃ§as
- âŒ ImpossÃ­vel fazer rollback
- âŒ SincronizaÃ§Ã£o manual entre dev/staging/prod

**O que precisa ser feito:**

#### 1.4.1 Instalar node-pg-migrate

```bash
npm install --save node-pg-migrate
npm install --save-dev @types/node-pg-migrate
```

#### 1.4.2 Configurar Migrations

Criar `.migrations.json`:
```json
{
  "databaseUrl": {
    "env": "DATABASE_URL"
  },
  "migrationsTable": "pgmigrations",
  "dir": "database/migrations",
  "checkOrder": true,
  "direction": "up",
  "log": true,
  "verbose": true
}
```

#### 1.4.3 Estrutura de Migrations

```
database/
â”œâ”€â”€ migrations/
â”‚   â”œâ”€â”€ 1699000000000_initial-schema.js
â”‚   â”œâ”€â”€ 1699000000001_add-users-table.js
â”‚   â”œâ”€â”€ 1699000000002_add-publishers-table.js
â”‚   â”œâ”€â”€ 1699000000003_add-titles-table.js
â”‚   â”œâ”€â”€ 1699000000004_add-issues-table.js
â”‚   â””â”€â”€ 1699000000005_add-ratings-favorites.js
â””â”€â”€ init.sql (deprecated - serÃ¡ substituÃ­do)
```

#### 1.4.4 Exemplo de Migration

Criar `database/migrations/1699000000001_add-users-table.js`:
```javascript
/* eslint-disable camelcase */

exports.shorthands = undefined;

exports.up = pgm => {
  // Create users table
  pgm.createTable('users', {
    id: {
      type: 'serial',
      primaryKey: true
    },
    name: {
      type: 'varchar(255)',
      notNull: true
    },
    email: {
      type: 'varchar(255)',
      notNull: true,
      unique: true
    },
    password_hash: {
      type: 'text',
      notNull: true
    },
    role: {
      type: 'varchar(50)',
      notNull: true,
      default: 'reader',
      check: "role IN ('admin', 'editor', 'reader')"
    },
    created_at: {
      type: 'timestamp',
      notNull: true,
      default: pgm.func('CURRENT_TIMESTAMP')
    },
    updated_at: {
      type: 'timestamp',
      notNull: true,
      default: pgm.func('CURRENT_TIMESTAMP')
    }
  });

  // Create indexes
  pgm.createIndex('users', 'email');
  pgm.createIndex('users', 'role');

  // Create updated_at trigger
  pgm.createFunction(
    'trigger_set_timestamp',
    [],
    {
      returns: 'TRIGGER',
      language: 'plpgsql',
      replace: true
    },
    `
    BEGIN
      NEW.updated_at = NOW();
      RETURN NEW;
    END;
    `
  );

  pgm.createTrigger('users', 'set_timestamp', {
    when: 'BEFORE',
    operation: 'UPDATE',
    function: 'trigger_set_timestamp',
    level: 'ROW'
  });
};

exports.down = pgm => {
  pgm.dropTable('users', { cascade: true });
  pgm.dropFunction('trigger_set_timestamp', [], { cascade: true });
};
```

#### 1.4.5 Atualizar package.json

```json
{
  "scripts": {
    "db:migrate": "node-pg-migrate up",
    "db:migrate:down": "node-pg-migrate down",
    "db:migrate:create": "node-pg-migrate create",
    "db:migrate:redo": "node-pg-migrate redo",
    "db:migrate:status": "node-pg-migrate status"
  }
}
```

#### 1.4.6 VariÃ¡vel de Ambiente

Adicionar ao `.env`:
```
DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}
```

#### 1.4.7 Migration Strategy

1. **Desenvolvimento:**
   - Create migration: `npm run db:migrate:create add-new-column`
   - Apply migration: `npm run db:migrate`
   - Test changes
   - Rollback if needed: `npm run db:migrate:down`

2. **Staging:**
   - Pull latest code with migrations
   - Run: `npm run db:migrate`
   - Test

3. **ProduÃ§Ã£o:**
   - Backup database first
   - Run: `npm run db:migrate`
   - Monitor application
   - Rollback plan: `npm run db:migrate:down`

**EsforÃ§o Estimado:** 24-32 horas
**Prioridade:** ğŸ”´ CRÃTICA (Bloqueador #4)

---

### 1.5 Monitoramento e Observabilidade (Severidade: CRÃTICA)

**Status Atual:** ğŸ”´ Logs bÃ¡sicos apenas

**Problema:**
- Apenas `console.log()` bÃ¡sico
- Sem agregaÃ§Ã£o de logs
- Sem mÃ©tricas de performance
- Sem alertas de incidentes
- Sem tracing distribuÃ­do

**Impacto:**
- âŒ DifÃ­cil debuggar problemas em produÃ§Ã£o
- âŒ Sem visibilidade de performance
- âŒ Incidentes descobertos por usuÃ¡rios
- âŒ MTTR (Mean Time To Recovery) alto

**O que precisa ser feito:**

#### 1.5.1 Structured Logging com Winston

```bash
npm install winston winston-daily-rotate-file
```

Atualizar `src/utils/logger.js`:
```javascript
const winston = require('winston');
const DailyRotateFile = require('winston-daily-rotate-file');

const logFormat = winston.format.combine(
  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),
  winston.format.errors({ stack: true }),
  winston.format.splat(),
  winston.format.json()
);

// Transport: Console
const consoleTransport = new winston.transports.Console({
  format: winston.format.combine(
    winston.format.colorize(),
    winston.format.printf(({ timestamp, level, message, ...meta }) => {
      return `${timestamp} [${level}]: ${message} ${
        Object.keys(meta).length ? JSON.stringify(meta, null, 2) : ''
      }`;
    })
  )
});

// Transport: Rotating File
const fileTransport = new DailyRotateFile({
  filename: 'logs/application-%DATE%.log',
  datePattern: 'YYYY-MM-DD',
  maxSize: '20m',
  maxFiles: '14d',
  format: logFormat
});

// Transport: Error File
const errorFileTransport = new DailyRotateFile({
  filename: 'logs/error-%DATE%.log',
  datePattern: 'YYYY-MM-DD',
  level: 'error',
  maxSize: '20m',
  maxFiles: '30d',
  format: logFormat
});

const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: logFormat,
  defaultMeta: {
    service: 'revista-cms-api',
    environment: process.env.NODE_ENV || 'development'
  },
  transports: [
    consoleTransport,
    fileTransport,
    errorFileTransport
  ]
});

// Add request ID to all logs
logger.addRequestContext = (req, res, next) => {
  req.id = require('crypto').randomUUID();
  req.logger = logger.child({ requestId: req.id });
  next();
};

module.exports = logger;
```

#### 1.5.2 Application Performance Monitoring (APM)

OpÃ§Ã£o 1: **Elastic APM** (Open Source)
```bash
npm install elastic-apm-node
```

Criar `src/apm.js`:
```javascript
const apm = require('elastic-apm-node').start({
  serviceName: 'revista-cms-api',
  serverUrl: process.env.ELASTIC_APM_SERVER_URL,
  secretToken: process.env.ELASTIC_APM_SECRET_TOKEN,
  environment: process.env.NODE_ENV,
  captureBody: 'errors',
  captureHeaders: true,
  logLevel: 'info'
});

module.exports = apm;
```

OpÃ§Ã£o 2: **New Relic**
```bash
npm install newrelic
```

OpÃ§Ã£o 3: **Datadog**
```bash
npm install dd-trace --save
```

#### 1.5.3 Prometheus Metrics

```bash
npm install prom-client
```

Criar `src/middleware/metrics.js`:
```javascript
const promClient = require('prom-client');

// Create a Registry
const register = new promClient.Registry();

// Add default metrics
promClient.collectDefaultMetrics({ register });

// Custom metrics
const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [0.1, 0.5, 1, 2, 5]
});

const httpRequestTotal = new promClient.Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status_code']
});

const databaseQueryDuration = new promClient.Histogram({
  name: 'db_query_duration_seconds',
  help: 'Duration of database queries in seconds',
  labelNames: ['query_type', 'table'],
  buckets: [0.01, 0.05, 0.1, 0.5, 1]
});

const activeConnections = new promClient.Gauge({
  name: 'db_active_connections',
  help: 'Number of active database connections'
});

register.registerMetric(httpRequestDuration);
register.registerMetric(httpRequestTotal);
register.registerMetric(databaseQueryDuration);
register.registerMetric(activeConnections);

// Middleware to track requests
const metricsMiddleware = (req, res, next) => {
  const start = Date.now();

  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    const route = req.route ? req.route.path : req.path;

    httpRequestDuration.labels(req.method, route, res.statusCode).observe(duration);
    httpRequestTotal.labels(req.method, route, res.statusCode).inc();
  });

  next();
};

module.exports = {
  register,
  metricsMiddleware,
  databaseQueryDuration,
  activeConnections
};
```

Adicionar endpoint de mÃ©tricas no `src/index.js`:
```javascript
const { register, metricsMiddleware } = require('./middleware/metrics');

app.use(metricsMiddleware);

app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});
```

#### 1.5.4 Health Check AvanÃ§ado

Atualizar `/health` endpoint:
```javascript
const pool = require('./config/database');

app.get('/health', async (req, res) => {
  const healthcheck = {
    uptime: process.uptime(),
    message: 'OK',
    timestamp: Date.now(),
    checks: {}
  };

  try {
    // Database check
    const dbStart = Date.now();
    const result = await pool.query('SELECT NOW()');
    const dbDuration = Date.now() - dbStart;

    healthcheck.checks.database = {
      status: 'up',
      responseTime: dbDuration,
      timestamp: result.rows[0].now
    };

    // Memory check
    const memUsage = process.memoryUsage();
    healthcheck.checks.memory = {
      status: memUsage.heapUsed < memUsage.heapTotal * 0.9 ? 'up' : 'warning',
      heapUsed: `${Math.round(memUsage.heapUsed / 1024 / 1024)}MB`,
      heapTotal: `${Math.round(memUsage.heapTotal / 1024 / 1024)}MB`,
      usage: `${Math.round((memUsage.heapUsed / memUsage.heapTotal) * 100)}%`
    };

    // Connection pool check
    healthcheck.checks.connectionPool = {
      status: 'up',
      total: pool.totalCount,
      idle: pool.idleCount,
      waiting: pool.waitingCount
    };

    res.status(200).json(healthcheck);
  } catch (error) {
    healthcheck.message = 'ERROR';
    healthcheck.checks.database = {
      status: 'down',
      error: error.message
    };
    res.status(503).json(healthcheck);
  }
});

// Readiness probe
app.get('/ready', async (req, res) => {
  try {
    await pool.query('SELECT 1');
    res.status(200).json({ status: 'ready' });
  } catch (error) {
    res.status(503).json({ status: 'not ready', error: error.message });
  }
});

// Liveness probe
app.get('/live', (req, res) => {
  res.status(200).json({ status: 'alive' });
});
```

#### 1.5.5 Alerting com Grafana

Criar `monitoring/grafana-dashboard.json` e `monitoring/alerts.yml`:

```yaml
groups:
  - name: revista-cms-api
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status_code=~"5.."}[5m]))
          / sum(rate(http_requests_total[5m])) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # High response time
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            rate(http_request_duration_seconds_bucket[5m])
          ) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "P95 latency is {{ $value }}s"

      # Database connection issues
      - alert: DatabaseConnectionPoolExhausted
        expr: db_active_connections > 18
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "Active connections: {{ $value }}/20"

      # High memory usage
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes > 500000000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage: {{ $value | humanize }}B"
```

#### 1.5.6 Stack de Monitoramento Recomendada

**OpÃ§Ã£o 1: ELK Stack (Open Source)**
- **Elasticsearch**: Armazenamento de logs
- **Logstash**: Processamento de logs
- **Kibana**: VisualizaÃ§Ã£o

**OpÃ§Ã£o 2: PLG Stack (Open Source)**
- **Promtail**: Coleta de logs
- **Loki**: Armazenamento de logs
- **Grafana**: VisualizaÃ§Ã£o + Prometheus para mÃ©tricas

**OpÃ§Ã£o 3: Cloud Services**
- **Datadog**: All-in-one (pago)
- **New Relic**: APM + Logs (pago)
- **Elastic Cloud**: ELK managed (pago)

**EsforÃ§o Estimado:** 40-60 horas
**Prioridade:** ğŸ”´ CRÃTICA (Bloqueador #5)

---

## ğŸŸ¡ CATEGORIA 2: IMPORTANTES (NÃ£o Bloqueadores)

### 2.1 DocumentaÃ§Ã£o de API (Swagger/OpenAPI)

**Status Atual:** ğŸŸ¡ Apenas Postman collection

**EsforÃ§o:** 16-20 horas
**Prioridade:** ğŸ”¶ ALTA

#### O que fazer:

```bash
npm install swagger-ui-express swagger-jsdoc
```

Criar `src/config/swagger.js`:
```javascript
const swaggerJsdoc = require('swagger-jsdoc');

const options = {
  definition: {
    openapi: '3.0.0',
    info: {
      title: 'Revista CMS API',
      version: '1.0.0',
      description: 'API RESTful para gerenciamento de revistas e quadrinhos',
      contact: {
        name: 'API Support',
        email: 'support@theoldshinobi.com'
      }
    },
    servers: [
      {
        url: 'http://localhost:3000',
        description: 'Development server'
      },
      {
        url: 'https://api.theoldshinobi.com',
        description: 'Production server'
      }
    ],
    components: {
      securitySchemes: {
        bearerAuth: {
          type: 'http',
          scheme: 'bearer',
          bearerFormat: 'JWT'
        }
      }
    }
  },
  apis: ['./src/routes/*.js', './src/controllers/*.js']
};

const specs = swaggerJsdoc(options);

module.exports = specs;
```

Adicionar ao `src/index.js`:
```javascript
const swaggerUi = require('swagger-ui-express');
const swaggerSpecs = require('./config/swagger');

app.use('/api-docs', swaggerUi.serve, swaggerUi.setup(swaggerSpecs));
```

---

### 2.2 Backup e Disaster Recovery

**Status Atual:** ğŸ”´ Inexistente

**EsforÃ§o:** 24-32 horas
**Prioridade:** ğŸ”¶ ALTA

#### EstratÃ©gia de Backup:

1. **Database Backups Automatizados**

Criar `scripts/backup-db.sh`:
```bash
#!/bin/bash
set -e

BACKUP_DIR="/backups/postgresql"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/revista_backup_$TIMESTAMP.sql"

# Create backup directory
mkdir -p $BACKUP_DIR

# Dump database
pg_dump -h $DB_HOST -U $DB_USER -d $DB_NAME \
  --format=custom \
  --compress=9 \
  --file=$BACKUP_FILE

# Upload to S3
aws s3 cp $BACKUP_FILE s3://revista-backups/database/

# Keep only last 7 days locally
find $BACKUP_DIR -name "*.sql" -mtime +7 -delete

echo "âœ… Backup completed: $BACKUP_FILE"
```

Agendar com cron:
```cron
# Backup diÃ¡rio Ã s 2 AM
0 2 * * * /opt/revista-cms-api/scripts/backup-db.sh

# Backup de uploads semanalmente
0 3 * * 0 rsync -avz /opt/revista-cms-api/uploads/ s3://revista-backups/uploads/
```

2. **Disaster Recovery Plan**

Criar `docs/disaster-recovery.md`:
```markdown
# Disaster Recovery Plan

## RTO (Recovery Time Objective): 4 hours
## RPO (Recovery Point Objective): 24 hours

### Scenario 1: Database Corruption
1. Stop application
2. Restore from latest backup
3. Run migrations
4. Restart application
5. Validate data integrity

### Scenario 2: Complete Server Failure
1. Provision new server
2. Deploy application via Docker
3. Restore database backup
4. Restore uploads from S3
5. Update DNS
6. Validate functionality
```

---

### 2.3 Secrets Management

**Status Atual:** ğŸŸ¡ .env files

**EsforÃ§o:** 12-16 horas
**Prioridade:** ğŸ”¶ ALTA

#### OpÃ§Ãµes:

**OpÃ§Ã£o 1: HashiCorp Vault**
```bash
npm install node-vault
```

**OpÃ§Ã£o 2: AWS Secrets Manager**
```bash
npm install @aws-sdk/client-secrets-manager
```

**OpÃ§Ã£o 3: Doppler (mais simples)**
```bash
npm install --save-dev @doppler/cli
doppler setup
doppler run -- npm start
```

---

### 2.4 Rate Limiting AvanÃ§ado

**Status Atual:** ğŸŸ¢ BÃ¡sico implementado

**Melhorias necessÃ¡rias:**
- Rate limiting baseado em usuÃ¡rio
- Rate limiting distribuÃ­do (Redis)
- Diferentes limites por plano/subscription

---

### 2.5 Caching Strategy

**Status Atual:** ğŸ”´ Sem cache

**EsforÃ§o:** 20-24 horas
**Prioridade:** ğŸ”¶ MÃ‰DIA

#### Implementar Redis Cache:

```bash
npm install redis ioredis
```

Criar `src/utils/cache.js`:
```javascript
const Redis = require('ioredis');

const redis = new Redis({
  host: process.env.REDIS_HOST,
  port: process.env.REDIS_PORT,
  password: process.env.REDIS_PASSWORD,
  retryStrategy: (times) => {
    const delay = Math.min(times * 50, 2000);
    return delay;
  }
});

class Cache {
  async get(key) {
    const data = await redis.get(key);
    return data ? JSON.parse(data) : null;
  }

  async set(key, value, ttl = 3600) {
    await redis.setex(key, ttl, JSON.stringify(value));
  }

  async del(key) {
    await redis.del(key);
  }

  async flush() {
    await redis.flushdb();
  }
}

module.exports = new Cache();
```

Middleware de cache:
```javascript
const cache = require('../utils/cache');

const cacheMiddleware = (duration = 300) => {
  return async (req, res, next) => {
    if (req.method !== 'GET') {
      return next();
    }

    const key = `cache:${req.originalUrl}`;
    const cachedResponse = await cache.get(key);

    if (cachedResponse) {
      return res.json(cachedResponse);
    }

    const originalJson = res.json.bind(res);
    res.json = (body) => {
      cache.set(key, body, duration);
      return originalJson(body);
    };

    next();
  };
};

module.exports = cacheMiddleware;
```

---

## ğŸ“Š RESUMO DE PRIORIDADES

### ğŸ”´ BLOQUEADORES CRÃTICOS (FASE 1)
Estimativa total: **132-192 horas** (3-5 semanas)

1. âœ… Testes Automatizados (40-60h)
2. âœ… CI/CD Pipeline (16-24h)
3. âœ… Docker/ContainerizaÃ§Ã£o (12-16h)
4. âœ… Database Migrations (24-32h)
5. âœ… Monitoramento (40-60h)

### ğŸ”¶ IMPORTANTES (FASE 2)
Estimativa total: **88-116 horas** (2-3 semanas)

6. Swagger/OpenAPI Documentation (16-20h)
7. Backup & Disaster Recovery (24-32h)
8. Secrets Management (12-16h)
9. Caching com Redis (20-24h)
10. Security Scanning (16-24h)

### ğŸŸ¢ MELHORIAS (FASE 3)
Estimativa total: **40-60 horas** (1-2 semanas)

11. Feature Flags
12. API Versioning
13. WebSockets para real-time
14. GraphQL endpoint (opcional)
15. Admin Dashboard

---

## ğŸ¯ ROADMAP RECOMENDADO

### Sprint 1 (Semana 1-2): Testing Foundation
- [ ] Configurar Jest e Supertest
- [ ] Escrever testes unitÃ¡rios para Services
- [ ] Escrever testes unitÃ¡rios para Repositories
- [ ] Atingir 80% de cobertura

### Sprint 2 (Semana 3-4): Testing & CI/CD
- [ ] Escrever testes de integraÃ§Ã£o
- [ ] Configurar GitHub Actions
- [ ] Configurar proteÃ§Ã£o de branches
- [ ] Adicionar badges de status

### Sprint 3 (Semana 5): ContainerizaÃ§Ã£o
- [ ] Criar Dockerfile
- [ ] Criar docker-compose
- [ ] Testar builds localmente
- [ ] Documentar processo

### Sprint 4 (Semana 6-7): Database Migrations
- [ ] Configurar node-pg-migrate
- [ ] Criar migrations do schema atual
- [ ] Testar rollbacks
- [ ] Documentar processo

### Sprint 5 (Semana 8-9): Monitoramento
- [ ] Configurar Winston logging
- [ ] Adicionar Prometheus metrics
- [ ] Configurar health checks avanÃ§ados
- [ ] Setup Grafana dashboards
- [ ] Configurar alertas

### Sprint 6 (Semana 10): DocumentaÃ§Ã£o & Backups
- [ ] Adicionar Swagger docs
- [ ] Configurar backups automatizados
- [ ] Criar runbooks
- [ ] Disaster recovery testing

### Sprint 7 (Semana 11): Final Polish
- [ ] Secrets management
- [ ] Caching strategy
- [ ] Performance testing
- [ ] Security audit final

---

## âœ… CHECKLIST FINAL ANTES DE PRODUÃ‡ÃƒO

### SeguranÃ§a
- [ ] Todas as vulnerabilidades crÃ­ticas resolvidas
- [ ] Secrets nÃ£o estÃ£o no cÃ³digo
- [ ] Rate limiting configurado
- [ ] CORS configurado corretamente
- [ ] Security headers (Helmet) ativo
- [ ] Input validation em todos endpoints
- [ ] SQL injection prevenida
- [ ] Audit de dependÃªncias passing

### Testing
- [ ] Cobertura de testes > 80%
- [ ] Todos os testes passando
- [ ] Testes de integraÃ§Ã£o implementados
- [ ] Testes E2E para fluxos crÃ­ticos
- [ ] Performance testing realizado

### Infrastructure
- [ ] Docker images building
- [ ] docker-compose funcionando
- [ ] CI/CD pipeline verde
- [ ] Database migrations funcionando
- [ ] Backups automatizados configurados
- [ ] Disaster recovery testado

### Monitoramento
- [ ] Logs estruturados implementados
- [ ] MÃ©tricas coletadas (Prometheus)
- [ ] Health checks funcionando
- [ ] Dashboards criados (Grafana)
- [ ] Alertas configurados
- [ ] Runbooks escritos

### DocumentaÃ§Ã£o
- [ ] README.md atualizado
- [ ] API documentada (Swagger)
- [ ] Guia de deploy escrito
- [ ] Runbooks de operaÃ§Ã£o
- [ ] Disaster recovery plan
- [ ] Arquitetura documentada

### Compliance
- [ ] LGPD/GDPR considerations
- [ ] Logs nÃ£o contÃªm dados sensÃ­veis
- [ ] PolÃ­tica de retenÃ§Ã£o de dados
- [ ] Terms of Service
- [ ] Privacy Policy

---

## ğŸš¦ DECISÃƒO FINAL

### âŒ SISTEMA NÃƒO ESTÃ PRONTO PARA PRODUÃ‡ÃƒO

**RazÃ£o:** 5 bloqueadores crÃ­ticos impedem deploy seguro

**Estimativa para produÃ§Ã£o:** **11-16 semanas** de trabalho

**RecomendaÃ§Ã£o:**
1. **Imediato (Fase 1):** Resolver bloqueadores crÃ­ticos (5 semanas)
2. **Curto prazo (Fase 2):** Implementar itens importantes (3 semanas)
3. **MÃ©dio prazo (Fase 3):** Melhorias e otimizaÃ§Ãµes (2 semanas)

**MVP MÃ­nimo para ProduÃ§Ã£o (Fase 1 apenas):**
- âœ… Testes com 80% cobertura
- âœ… CI/CD funcional
- âœ… Docker + docker-compose
- âœ… Database migrations
- âœ… Monitoramento bÃ¡sico

ApÃ³s Fase 1 completa, o sistema pode ser deployed em produÃ§Ã£o com **monitoramento muito prÃ³ximo** e **disponibilidade limitada** (soft launch/beta).

---

## ğŸ“ PRÃ“XIMOS PASSOS

1. **Revisar este documento** com stakeholders
2. **Priorizar itens** baseado em business needs
3. **Alocar recursos** (tempo/pessoas)
4. **Criar sprint planning** detalhado
5. **ComeÃ§ar pela Fase 1** imediatamente

**Ãšltima atualizaÃ§Ã£o:** 07/11/2025
**RevisÃ£o necessÃ¡ria:** ApÃ³s cada sprint
